# app.py â€” AJD Topic Explorer (Bilingual EN/AR) â€” 2025-08
# Features:
# - Robust CSV loaders + natural-sort merge of part files
# - Bilingual UI (English / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©) with RTL when Arabic is selected
# - Tabs: Search, Compare, Similarity (optional), Loglines (strand presets + short+ detailed), Diagnostics
# - Unique widget keys to avoid Streamlit KeyError; Reset UI state; st.rerun()

import re, json, glob, os
from pathlib import Path
from typing import List, Tuple

import pandas as pd
import streamlit as st
import os

# Optional (Similarity tab). App still runs if sklearn isn't available.
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_OK = True
except Exception:
    SKLEARN_OK = False

# Attempt to import OpenAI if available; used for AI-powered loglines
from openai import OpenAI
import os

api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)  # optional, defaults to reading from environment

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": prompt_en}],
    max_tokens=800,
    temperature=0.7,
)
content = response.choices[0].message.content

APP_TITLE = "AJD Topic Explorer â€” Ø¨Ø­Ø« Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ©"
DATA_DIR = Path("data")

# ============================ Bilingual helpers ============================

def get_lang() -> str:
    return st.session_state.get("sb_lang", "English")

def L(en: str, ar: str) -> str:
    return ar if get_lang() == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else en

def apply_rtl_if_arabic():
    if get_lang() == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
        st.markdown(
            """
            <style>
            html, body, [data-testid="stAppViewContainer"], [data-testid="stSidebar"] * {
                direction: rtl;
                text-align: right;
                font-family: "Noto Naskh Arabic", "Amiri", "Tahoma", sans-serif !important;
            }
            [data-testid="stTable"] thead tr th, [data-testid="stDataFrame"] thead tr th {
                text-align: right !important;
            }
            </style>
            """,
            unsafe_allow_html=True,
        )

# ============================ Data helpers ============================

def _natsort_key(path: str):
    base = os.path.basename(path)
    return [int(t) if t.isdigit() else t.lower() for t in re.findall(r"\d+|\D+", base)]

def _merge_chunked_csv(pattern: str, merged_path: Path) -> bool:
    """Merge chunked CSVs (part01, part02, part3_01, ...) using natural sort."""
    parts = sorted(glob.glob(pattern), key=_natsort_key)
    if not parts:
        return False
    merged_path.parent.mkdir(parents=True, exist_ok=True)
    with open(merged_path, "w", encoding="utf-8") as out:
        for i, p in enumerate(parts):
            with open(p, "r", encoding="utf-8") as f:
                if i == 0:
                    out.write(f.read())     # header + data
                else:
                    _ = f.readline()        # skip header
                    out.write(f.read())     # data only
    return True

@st.cache_data(show_spinner=False)
def load_topics_df() -> pd.DataFrame:
    """Load topics CSV (or auto-merge parts). Returns columns: topic, count (strings)."""
    p = DATA_DIR / "ajd_topics_extracted.csv"
    if not p.exists():
        _merge_chunked_csv(str(DATA_DIR / "ajd_topics_extracted.part*.csv"), p)
    if not p.exists():
        return pd.DataFrame(columns=["topic", "count"])
    try:
        df = pd.read_csv(p, dtype=str, na_filter=False, keep_default_na=False,
                         engine="python", on_bad_lines="skip")
    except Exception:
        return pd.DataFrame(columns=["topic", "count"])
    cols_lower = [c.strip().lower() for c in df.columns]
    if "topic" in cols_lower and "count" in cols_lower:
        df = df.rename(columns={
            df.columns[cols_lower.index("topic")]: "topic",
            df.columns[cols_lower.index("count")]: "count",
        })
        return df[["topic", "count"]]
    if len(df.columns) >= 2:
        return df.rename(columns={df.columns[0]: "topic", df.columns[1]: "count"})[["topic", "count"]]
    return pd.DataFrame(columns=["topic", "count"])

@st.cache_data(show_spinner=False)
def load_catalogue_df() -> pd.DataFrame:
    """Robust loader: auto-merge parts; try several encodings & delimiters; clean columns/rows."""
    p = DATA_DIR / "ajd_catalogue_raw.csv"
    if not p.exists():
        _merge_chunked_csv(str(DATA_DIR / "ajd_catalogue_raw.part*.csv"), p)
    if not p.exists():
        return pd.DataFrame()

    encodings = ["utf-8", "utf-8-sig", "cp1256", "latin1"]
    seps = [None, ",", ";", "\t", "|"]  # None => pandas sniff
    df = None
    last_err = None
    for enc in encodings:
        for sep in seps:
            try:
                df_try = pd.read_csv(
                    p, sep=sep, encoding=enc, engine="python",
                    dtype=str, na_filter=False, keep_default_na=False,
                    on_bad_lines="skip"
                )
                if df_try is not None and df_try.shape[0] > 0:
                    df = df_try
                    break
            except Exception as e:
                last_err = e
        if df is not None:
            break

    if df is None:
        st.error(L(f"Could not read catalogue CSV. Last error: {last_err}",
                   f"ØªØ¹Ø°Ù‘Ø± Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³. Ø¢Ø®Ø± Ø®Ø·Ø£: {last_err}"))
        return pd.DataFrame()

    # Normalize column names
    df.columns = [re.sub(r"\s+", " ", str(c)).strip() for c in df.columns]

    # Drop fully empty columns and rows
    df = df.dropna(axis=1, how="all")
    if not df.empty:
        df = df.loc[~(df.apply(lambda r: "".join(map(str, r)).strip(), axis=1) == "")]

    # Quick shape (sidebar)
    st.sidebar.write(L("Loaded catalogue:","ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³:"),
                     f"{df.shape[0]:,} {L('rows','Ø³Ø·Ø±')} Ã— {df.shape[1]} {L('cols','Ø¹Ù…ÙˆØ¯')}")
    return df

@st.cache_data(show_spinner=False)
def infer_text_columns(df: pd.DataFrame) -> List[str]:
    patt = re.compile(r"(title|synopsis|summary|topic|subject|tags|genre|theme|series|strand|category|type|desc|name|english|arabic|Ø§Ø³Ù…|Ø¹Ù†ÙˆØ§Ù†|Ù…Ù„Ø®|Ù…ÙˆØ¶ÙˆØ¹|Ø³Ù„Ø³Ù„Ø©|Ø¹Ø±Ø¨ÙŠ)", re.I)
    return [c for c in df.columns if patt.search(str(c))]

def _clean_hidden_series(s: pd.Series) -> pd.Series:
    """Remove zero-width/bidi chars and normalize spaces."""
    ZW_PATTERN = re.compile(r"[\u200B-\u200F\u202A-\u202E\u2066-\u2069]")
    WS_PATTERN = re.compile(r"\s+")
    return s.astype(str).fillna("").map(lambda x: WS_PATTERN.sub(" ", ZW_PATTERN.sub("", x)).strip())

# ============================ App ============================

def main() -> None:
    st.set_page_config(page_title=APP_TITLE, layout="wide")
    st.title("AJD Topic Explorer â€” Ù„ÙˆØ­Ø© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ù„ØºØ©")
    st.caption(L(
        "Search the AJD catalogue, compare topics, find matches, and craft bilingual loglines.",
        "Ø§Ø¨Ø­Ø« ÙÙŠ ÙÙ‡Ø±Ø³ AJDØŒ Ù‚Ø§Ø±Ù† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§ØªØŒ Ø§Ø¹Ø«Ø± Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚Ø§ØªØŒ ÙˆØ£Ù†Ø´Ø¦ Ù„ÙˆØ¬Ù„Ø§ÙŠÙ† Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ©."
    ))

    # ----- Sidebar -----
    with st.sidebar:
        st.header(L("Dataset","Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"))
        # Language switch + RTL
        ui_lang = st.selectbox("Language / Ø§Ù„Ù„ØºØ©", ["English", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"], key="sb_lang")
        apply_rtl_if_arabic()

        topics_df = load_topics_df()
        cat_df = load_catalogue_df()
        st.write(f"**{L('Topics','Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª')}**", f"{len(topics_df):,} {L('rows','Ø³Ø·Ø±')}" if not topics_df.empty else L("â€” missing","â€” ØºÙŠØ± Ù…ØªÙˆÙØ±"))
        st.write(f"**{L('Catalogue','Ø§Ù„ÙÙ‡Ø±Ø³')}**", f"{len(cat_df):,} {L('rows','Ø³Ø·Ø±')}" if not cat_df.empty else L("â€” missing","â€” ØºÙŠØ± Ù…ØªÙˆÙØ±"))

        # Reset & Reload
        if st.button(L("Reset UI state (fix KeyError)","Ø¥Ø¹Ø§Ø¯Ø© Ø¶Ø¨Ø· ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"), key="sb_reset_state"):
            st.session_state.clear()
            st.rerun()

        if st.button(L("Reload data / clear cache","ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª / Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©"), key="sb_reload"):
            load_topics_df.clear(); load_catalogue_df.clear(); infer_text_columns.clear()
            st.rerun()

        if topics_df.empty or cat_df.empty:
            st.warning(L(
                "If merged CSVs are missing, the app will merge `/data/...partNN.csv` on first run.",
                "Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ù„ÙØ§Øª Ø§Ù„Ø¯Ù…Ø¬ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¯Ù…Ø¬ `/data/...partNN.csv` ÙÙŠ Ø£ÙˆÙ„ ØªØ´ØºÙŠÙ„."
            ))

    # ----- Tabs -----
    search_tab, compare_tab, similar_tab, logline_tab, diag_tab = st.tabs([
        L("ğŸ” Search AJD Catalogue","ğŸ” Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙÙ‡Ø±Ø³ AJD"),
        L("ğŸ” Topic Overlap","ğŸ” ØªÙ‚Ø§Ø·Ø¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª"),
        L("ğŸ§­ Similarity Matches","ğŸ§­ Ø£Ù‚Ø±Ø¨ ØªØ·Ø§Ø¨Ù‚Ø§Øª"),
        L("ğŸª„ Logline Suggestions","ğŸª„ Ø§Ù‚ØªØ±Ø§Ø­ Ù„ÙˆØ¬Ù„Ø§ÙŠÙ†"),
        L("ğŸ§° Diagnostics","ğŸ§° Ø§Ù„ØªØ´Ø®ÙŠØµ"),
    ])

    # ============================ TAB 1: Search ============================
    with search_tab:
        st.subheader(L("Search AJD Catalogue","Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙÙ‡Ø±Ø³ AJD"))
        cat_df = load_catalogue_df()
        if cat_df.empty:
            st.info(L("Upload/commit your catalogue CSV parts to `data/` and reload.",
                      "Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª CSV Ø§Ù„Ù…Ø¬Ø²Ù‘Ø£Ø© Ø¯Ø§Ø®Ù„ `data/` Ø«Ù… Ø­Ø¯Ù‘Ø« Ø§Ù„ØªØ·Ø¨ÙŠÙ‚."))
        else:
            text_cols_default = infer_text_columns(cat_df)
            if "diag_cols_preset" in st.session_state and not st.session_state["diag_cols_preset"]:
                del st.session_state["diag_cols_preset"]
            preset = st.session_state.get("diag_cols_preset")
            default_cols = preset if (preset and all(c in cat_df.columns for c in preset)) else (text_cols_default or list(cat_df.columns)[:5])

            cols = st.multiselect(L("Columns to search in:","Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ Ø§Ù„Ø¨Ø­Ø« ÙÙŠÙ‡Ø§:"),
                                  options=list(cat_df.columns), default=default_cols, key="search_cols")
            search_all = st.checkbox(L("Search ALL columns (ignore selection)","Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©"), value=False, key="search_all")
            fallback_combine = st.checkbox(L("Fallback: combine selected columns into one field","Ø¯Ù…Ø¬ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ Ø­Ù‚Ù„ ÙˆØ§Ø­Ø¯"), value=True, key="search_fallback")

            q = st.text_input(L("Keyword or phrase","ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ© Ø£Ùˆ Ø¹Ø¨Ø§Ø±Ø©"), key="search_q")
            mode = st.selectbox(L("Match mode","Ù†Ù…Ø· Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"),
                                [L("Contains (case-insensitive)","ÙŠØ­ØªÙˆÙŠ (Ø¨Ø¯ÙˆÙ† Ø­Ø³Ø§Ø³ÙŠØ© Ø­Ø§Ù„Ø©)"),
                                 L("Exact (case-insensitive)","ØªØ·Ø§Ø¨Ù‚ ØªØ§Ù… (Ø¨Ø¯ÙˆÙ† Ø­Ø³Ø§Ø³ÙŠØ© Ø­Ø§Ù„Ø©)"),
                                 "Regex"], index=0, key="search_mode")
            max_rows = st.slider(L("Max rows to show","Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø¹Ø±Ø¶"), 10, 2000, 200, key="search_max_rows")

            def _contains(series_or_text, q_, mode_):
                if isinstance(series_or_text, pd.Series):
                    s = _clean_hidden_series(series_or_text)
                    try:
                        if mode_.startswith("Contains") or mode_.startswith("ÙŠØ­ØªÙˆÙŠ"):
                            return s.str.contains(q_, case=False, na=False, regex=False)
                        elif mode_.startswith("Exact") or mode_.startswith("ØªØ·Ø§Ø¨Ù‚"):
                            return (s.str.strip().str.lower() == str(q_).strip().lower())
                        else:
                            return s.str.contains(q_, case=False, na=False, regex=True)
                    except Exception:
                        return pd.Series(False, index=s.index)
                else:
                    s = str(series_or_text)
                    if mode_.startswith("Contains") or mode_.startswith("ÙŠØ­ØªÙˆÙŠ"):
                        return q_.lower() in s.lower()
                    elif mode_.startswith("Exact") or mode_.startswith("ØªØ·Ø§Ø¨Ù‚"):
                        return s.strip().lower() == str(q_).strip().lower()
                    else:
                        try:
                            return re.search(q_, s, flags=re.I) is not None
                        except Exception:
                            return False

            # Quick Probe
            if st.button(L("Quick Probe (per-column matches)","ÙØ­Øµ Ø³Ø±ÙŠØ¹ (Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…ÙˆØ¯)"), key="search_quick_probe_btn"):
                if not q:
                    st.warning(L("Enter a query first.","Ø£Ø¯Ø®Ù„ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ù‹Ø§ Ø£ÙˆÙ„Ù‹Ø§."))
                else:
                    use_cols = list(cat_df.columns) if search_all or not cols else cols
                    out, nonempty = [], []
                    for c in use_cols:
                        s = _clean_hidden_series(cat_df[c])
                        try:
                            if mode.startswith("Contains") or mode.startswith("ÙŠØ­ØªÙˆÙŠ"):
                                n = s.str.contains(q, case=False, na=False, regex=False).sum()
                            elif mode.startswith("Exact") or mode.startswith("ØªØ·Ø§Ø¨Ù‚"):
                                n = (s.str.strip().str.lower() == str(q).strip().lower()).sum()
                            else:
                                n = s.str.contains(q, case=False, na=False, regex=True).sum()
                        except Exception:
                            n = 0
                        out.append((c, int(n)))
                        nonempty.append((c, int((s != "").sum())))
                    df_out = pd.DataFrame(out, columns=[L("column","Ø§Ù„Ø¹Ù…ÙˆØ¯"), L("matches","Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª")]).sort_values(L("matches","Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª"), ascending=False)
                    df_non = pd.DataFrame(nonempty, columns=[L("column","Ø§Ù„Ø¹Ù…ÙˆØ¯"), L("non_empty","ØºÙŠØ± ÙØ§Ø±Øº")]).sort_values(L("non_empty","ØºÙŠØ± ÙØ§Ø±Øº"), ascending=False)
                    st.write(L("**Matches per column (top 25):**","**Ø¹Ø¯Ø¯ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯ (Ø£Ø¹Ù„Ù‰ 25):**"))
                    st.dataframe(df_out.head(25), use_container_width=True)
                    st.write(L("**Non-empty cells per column (top 25):**","**Ø§Ù„Ø®Ù„Ø§ÙŠØ§ ØºÙŠØ± Ø§Ù„ÙØ§Ø±ØºØ© Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯ (Ø£Ø¹Ù„Ù‰ 25):**"))
                    st.dataframe(df_non.head(25), use_container_width=True)

            # Search
            if st.button(L("Search","Ø¨Ø­Ø«"), type="primary", key="search_go_btn"):
                if not q:
                    st.warning(L("Provide a query.","Ø£Ø¯Ø®Ù„ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ù‹Ø§."))
                else:
                    try:
                        use_cols = list(cat_df.columns) if search_all or not cols else cols
                        df2 = cat_df.copy()
                        for c in use_cols:
                            df2[c] = _clean_hidden_series(df2[c])

                        if fallback_combine:
                            combo = df2[use_cols].agg(" ".join, axis=1)
                            mask = _contains(combo, q, mode)
                        else:
                            mask = pd.Series(False, index=df2.index)
                            for c in use_cols:
                                mask |= _contains(df2[c], q, mode)

                        total = int(mask.sum())
                        res = df2[mask].head(max_rows)
                        st.success(L(f"Found {total:,} rows; showing {len(res):,}.",
                                     f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {total:,} ØµÙÙ‹Ø§Ø› Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶ {len(res):,}."))
                        if total == 0:
                            st.info(L("No matches. Use Quick Probe, try Search ALL columns, or switch modes.",
                                      "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬. Ø¬Ø±Ù‘Ø¨ Ø§Ù„ÙØ­Øµ Ø§Ù„Ø³Ø±ÙŠØ¹ Ø£Ùˆ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø£Ùˆ ØºÙŠÙ‘Ø± Ù†Ù…Ø· Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©."))
                        if not res.empty:
                            show_cols = use_cols if len(use_cols) <= 12 else use_cols[:12]
                            st.write(L("**Sample hits (chosen columns):**","**Ø¹ÙŠÙ†Ø§Øª Ù†ØªØ§Ø¦Ø¬ (Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©):**"))
                            st.dataframe(res[show_cols].head(20), use_container_width=True)
                            st.download_button(L("Download results (CSV)","ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (CSV)"),
                                               res.to_csv(index=False).encode("utf-8"),
                                               file_name="ajd_search_results.csv", mime="text/csv", key="search_dl")
                        else:
                            st.dataframe(res, use_container_width=True)
                    except Exception as e:
                        st.error(L(f"Search error: {e}", f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}"))

    # ============================ TAB 2: Compare ============================
    with compare_tab:
        st.subheader(L("Compare Your Project Topics vs AJD Topics","Ù…Ù‚Ø§Ø±Ù†Ø© Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø´Ø±ÙˆØ¹Ùƒ Ø¨Ù…ÙˆØ¶ÙˆØ¹Ø§Øª AJD"))
        topics_df = load_topics_df()
        uploaded = st.file_uploader(L("Upload your project file (CSV with a topics column, or JSON list)",
                                      "Ø§Ø±ÙØ¹ Ù…Ù„Ù Ù…Ø´Ø±ÙˆØ¹Ùƒ (CSV ÙŠØ­ØªÙˆÙŠ Ø¹Ù…ÙˆØ¯ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø£Ùˆ JSON)"),
                                    type=["csv","json"], key="cmp_upload")
        topics_col = st.text_input(L("If CSV, name of the column that contains topics (e.g., 'topics')",
                                     "Ø¥Ù† ÙƒØ§Ù† CSVØŒ Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª (Ù…Ø«Ù„ 'topics')"),
                                   value="topics", key="cmp_topics_col")

        if uploaded is not None:
            try:
                if uploaded.name.lower().endswith(".csv"):
                    p = pd.read_csv(uploaded, dtype=str, na_filter=False, keep_default_na=False, engine="python", on_bad_lines="skip")
                    st.write(L("Your CSV preview:","Ù…Ø¹Ø§ÙŠÙ†Ø© CSV:")); st.dataframe(p.head(10), use_container_width=True)
                    def explode_topics_str(text: str) -> List[str]:
                        parts = re.split(r"[;,/|Â·â€¢\-â€“â€”]+", str(text))
                        keepout = {"and","the","for","with","from","into","about","film","doc","docs","docu","series","episode","season","ajd","al jazeera","aljazeera",
                                   "Ùˆ","ÙÙŠ","Ù…Ù†","Ø¹Ù†","Ø¥Ù„Ù‰","Ø¹Ù„Ù‰","ÙÙŠÙ„Ù…","ÙˆØ«Ø§Ø¦Ù‚ÙŠ","Ø­Ù„Ù‚Ø©","Ø³Ù„Ø³Ù„Ø©","Ø§Ù„Ø¬Ø²ÙŠØ±Ø©","AJD"}
                        return [t.strip().lower() for t in parts if len(t.strip())>=3 and t.strip().lower() not in keepout]
                    bag: List[str] = []
                    if topics_col in p.columns:
                        for v in p[topics_col].fillna(""):
                            bag.extend(explode_topics_str(v))
                    else:
                        st.error(L(f"Column '{topics_col}' not found in your CSV.",
                                   f"Ø§Ù„Ø¹Ù…ÙˆØ¯ '{topics_col}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ CSV."))
                        bag = []
                else:
                    data = json.loads(uploaded.getvalue().decode("utf-8", errors="ignore"))
                    items = data if isinstance(data, list) else data.get("items", [])
                    def explode_topics_str(text: str) -> List[str]:
                        parts = re.split(r"[;,/|Â·â€¢\-â€“â€”]+", str(text))
                        keepout = {"and","the","for","with","from","into","about","film","doc","docs","docu","series","episode","season","ajd","al jazeera","aljazeera",
                                   "Ùˆ","ÙÙŠ","Ù…Ù†","Ø¹Ù†","Ø¥Ù„Ù‰","Ø¹Ù„Ù‰","ÙÙŠÙ„Ù…","ÙˆØ«Ø§Ø¦Ù‚ÙŠ","Ø­Ù„Ù‚Ø©","Ø³Ù„Ø³Ù„Ø©","Ø§Ù„Ø¬Ø²ÙŠØ±Ø©","AJD"}
                        return [t.strip().lower() for t in parts if len(t.strip())>=3 and t.strip().lower() not in keepout]
                    bag: List[str] = []
                    for item in items:
                        topics_val = item.get("topics") or item.get("tags") or ""
                        if isinstance(topics_val, list):
                            for t in topics_val: bag.extend(explode_topics_str(t))
                        else:
                            bag.extend(explode_topics_str(topics_val))

                if topics_df.empty:
                    st.error(L("AJD topics dataset is missing.","Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ¶ÙˆØ¹Ø§Øª AJD ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©."))
                else:
                    ajd_topics = set(topics_df["topic"].astype(str).str.lower())
                    proj_topics = set(bag)
                    overlap = sorted(list(ajd_topics & proj_topics))
                    only_proj = sorted(list(proj_topics - ajd_topics))

                    st.markdown(L("### Summary","### Ø§Ù„Ù…Ù„Ø®Ù‘Øµ"))
                    st.json({L("ajd_unique_topics","Ø¹Ø¯Ø¯ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª AJD Ø§Ù„ÙØ±ÙŠØ¯Ø©"): len(ajd_topics),
                             L("project_unique_topics","Ø¹Ø¯Ø¯ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø´Ø±ÙˆØ¹Ùƒ"): len(proj_topics),
                             L("overlap_count","Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹Ø§Øª"): len(overlap)})

                    c1, c2 = st.columns(2)
                    with c1:
                        st.markdown(L("**Overlap topics**","**Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…ØªÙ‚Ø§Ø·Ø¹Ø©**"))
                        st.dataframe(pd.DataFrame({"topic": overlap}), use_container_width=True)
                    with c2:
                        st.markdown(L("**Project-only topics (white space)**","**Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…ØªØ§Ø­Ø© Ù„Ù…Ø´Ø±ÙˆØ¹Ùƒ ÙÙ‚Ø·**"))
                        st.dataframe(pd.DataFrame({"topic": only_proj}), use_container_width=True)

                    st.download_button(L("Download overlap (CSV)","ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ (CSV)"),
                        pd.DataFrame({"topic": overlap}).to_csv(index=False).encode("utf-8"),
                        file_name="overlap_topics.csv", key="cmp_dl_overlap")
                    st.download_button(L("Download project-only (CSV)","ØªØ­Ù…ÙŠÙ„ (Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø´Ø±ÙˆØ¹Ùƒ ÙÙ‚Ø·)"),
                        pd.DataFrame({"topic": only_proj}).to_csv(index=False).encode("utf-8"),
                        file_name="project_only_topics.csv", key="cmp_dl_projonly")
            except Exception as e:
                st.error(L(f"Error processing upload: {e}", f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„: {e}"))

    # ============================ TAB 3: Similarity (optional) ============================
    with similar_tab:
        st.subheader(L("Find Closest AJD Matches for Your Films (TF-IDF)","Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ù‚Ø±Ø¨ ØªØ·Ø§Ø¨Ù‚Ø§Øª Ù„Ø£ÙÙ„Ø§Ù…Ùƒ (TF-IDF)"))
        if not SKLEARN_OK:
            st.info(L("Install scikit-learn to enable this tab (add `scikit-learn>=1.3,<2.0` to requirements.txt).",
                      "Ø«Ø¨Ù‘Øª scikit-learn Ù„ØªÙØ¹ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„ØµÙØ­Ø© (Ø£Ø¶ÙÙ `scikit-learn>=1.3,<2.0`)."))
        else:
            cat_df = load_catalogue_df()
            if cat_df.empty:
                st.info(L("Catalogue missing. Add CSVs to /data and reload.",
                          "Ø§Ù„ÙÙ‡Ø±Ø³ ØºÙŠØ± Ù…ØªÙˆÙØ±. Ø£Ø¶ÙÙ Ù…Ù„ÙØ§Øª CSV Ø¥Ù„Ù‰ /data Ø«Ù… Ø­Ø¯Ù‘Ø«."))
            else:
                text_cols_default = infer_text_columns(cat_df)
                preset = st.session_state.get("diag_cols_preset")
                default_cols = preset if (preset and all(c in cat_df.columns for c in preset)) else (text_cols_default or list(cat_df.columns)[:5])

                cols = st.multiselect(L("AJD text columns to use","Ø£Ø¹Ù…Ø¯Ø© Ù†ØµÙŠØ© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§"), options=list(cat_df.columns), default=default_cols, key="sim_cols")
                films = st.text_area(L("Paste your films as JSON list (title, description, topics optional)",
                                       "Ø£Ù„ØµÙ‚ Ø£ÙÙ„Ø§Ù…Ùƒ ÙƒÙ‚Ø§Ø¦Ù…Ø© JSON (Ø¹Ù†ÙˆØ§Ù†ØŒ ÙˆØµÙØŒ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø§Ø®ØªÙŠØ§Ø±ÙŠ)"),
                    value='[\n  {"title":"Film A","description":"Inside the rise of tech startups in MENA."},\n  {"title":"Film B","description":"Lives split across continents."}\n]',
                    key="sim_films_text")
                compute_clicked = st.button(L("Compute matches","Ø§Ø­Ø³Ø¨ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª"), type="primary", key="sim_compute_btn")
                if compute_clicked:
                    try:
                        films_data = json.loads(films)
                        proj_texts = [(i.get("title","Untitled"), (i.get("title","")+" "+i.get("description"," ")).strip())
                                      for i in films_data]
                        ajd_texts = (cat_df[cols].fillna("").astype(str).agg(" ".join, axis=1)).tolist()
                        vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2))
                        X = vectorizer.fit_transform(ajd_texts + [t for _, t in proj_texts])
                        n_ajd = len(ajd_texts)
                        sims = cosine_similarity(X[n_ajd:], X[:n_ajd])

                        for row_idx, (title, _) in enumerate(proj_texts):
                            st.markdown(f"#### {title}")
                            row = sims[row_idx]
                            top_idx = row.argsort()[::-1][:10]
                            rows = []
                            for j in top_idx:
                                score = float(row[j]); ajd_row = cat_df.iloc[j]
                                rows.append({"score": round(score, 4), **ajd_row.to_dict()})
                            out_df = pd.DataFrame(rows)
                            st.dataframe(out_df, use_container_width=True)
                            safe_name = re.sub(r"[^A-Za-z0-9]+","_", str(title).lower())
                            st.download_button(L(f"Download matches for {title}","ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"),
                                out_df.to_csv(index=False).encode("utf-8"),
                                file_name=f"matches_{safe_name}.csv", key=f"sim_dl_{row_idx}")
                    except Exception as e:
                        st.error(L(f"Invalid JSON or error computing similarities: {e}",
                                   f"JSON ØºÙŠØ± ØµØ§Ù„Ø­ Ø£Ùˆ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­Ø³Ø§Ø¨: {e}"))

    # ============================ TAB 4: Loglines (Bilingual + Presets) ============================
    with logline_tab:
        # UI labels depend on language
        st.subheader(L("Suggest Strong Loglines","Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ¬Ù„Ø§ÙŠÙ† Ù‚ÙˆÙŠ"))
        st.caption(L(
            "Generates a tight tagline + a detailed commissioning logline, with presets & anti-clichÃ©.",
            "ÙŠÙ†ØªØ¬ Ø´Ø¹Ø§Ø±Ù‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ Ø¬Ø°Ø§Ø¨Ù‹Ø§ + Ù„ÙˆØ¬Ù„Ø§ÙŠÙ† ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ø¹Ø±Ø¶ØŒ Ù…Ø¹ Ù‚ÙˆØ§Ù„Ø¨ Ø¬Ø§Ù‡Ø²Ø© ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ù„Ù„ÙƒÙ„ÙŠØ´ÙŠÙ‡Ø§Øª."
        ))

        seed = st.text_area(
            L("Describe your film (topic, access, character, tension) â€” 2â€“5 lines.",
              "ØµÙ ÙÙŠÙ„Ù…Ùƒ (Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŒ Ø§Ù„ÙˆØµÙˆÙ„ØŒ Ø§Ù„Ø´Ø®ØµÙŠØ©ØŒ Ø§Ù„ØªÙˆØªØ±) â€” Ø³Ø·Ø±Ø§Ù† Ø¥Ù„Ù‰ Ø®Ù…Ø³Ø©."),
            key="log_seed",
            placeholder=L(
                "Ex: A Cairo paramedic livestreams night shifts; the feed exposes a black-market oxygen ring and a choice that could cost him his licenseâ€”or a life.",
                "Ù…Ø«Ø§Ù„: Ù…Ø³Ø¹Ù ÙÙŠ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© ÙŠØ¨Ø« Ù†ÙˆØ¨Ø§Øª Ø§Ù„Ù„ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø±Ø©Ø› ÙŠÙƒØ´Ù Ø§Ù„Ø¨Ø« Ø´Ø¨ÙƒØ© Ø£ÙƒØ³Ø¬ÙŠÙ† Ø³ÙˆØ¯Ø§Ø¡ ÙˆØ®ÙŠØ§Ø±Ù‹Ø§ Ù‚Ø¯ ÙŠÙƒÙ„Ù‘ÙÙ‡ Ø±Ø®ØµØªÙ‡ â€” Ø£Ùˆ Ø­ÙŠØ§Ø©."
            )
        )

        preset = st.selectbox(
            L("Strand preset (optional)","Ù†Ù…Ø· Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬/Ø§Ù„Ø³Ù„Ø³Ù„Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)"),
            [
                L("Custom","Ù…Ø®ØµØµ"),
                L("Witness / Observational","ÙˆÙŠØªÙ†Ø³ / Ø±ØµØ¯ÙŠ"),
                L("Investigative (doc-forward)","ØªØ­Ù‚ÙŠÙ‚ÙŠ (ÙŠØ¹ØªÙ…Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚)"),
                L("Current Affairs (system vs individual)","Ø´Ø¤ÙˆÙ† Ø¬Ø§Ø±ÙŠØ© (Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„ÙØ±Ø¯)"),
                L("Archive-led / Past in Present","Ø£Ø±Ø´ÙŠÙÙŠ / Ø§Ù„Ù…Ø§Ø¶ÙŠ ÙÙŠ Ø§Ù„Ø­Ø§Ø¶Ø±"),
                L("Exclusive Access","ÙˆØµÙˆÙ„ Ø­ØµØ±ÙŠ"),
                L("Countdown / Clock","Ø¹Ø¯Ù‘ ØªÙ†Ø§Ø²Ù„ÙŠ / Ø³Ø¨Ø§Ù‚ Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª"),
            ],
            key="log_preset"
        )

        tone = st.selectbox(
            L("Tone / voice","Ø§Ù„Ù†Ø¨Ø±Ø© / Ø§Ù„Ø£Ø³Ù„ÙˆØ¨"),
            ["Neutral", "Urgent", "Gritty", "Poetic"] if get_lang()=="English" else ["Ù…Ø­Ø§ÙŠØ¯", "Ø¹Ø§Ø¬Ù„", "Ø®Ø´Ù†", "Ø´Ø¹Ø±ÙŠ"],
            index=1, key="log_tone"
        )

        angle_pack = st.multiselect(
            L("Angles to try","Ø²ÙˆØ§ÙŠØ§ Ø§Ù„Ø³Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©"),
            (
                ["Character (inside-out)", "System vs Individual", "Investigation / Leak", "Exclusive Access",
                 "Countdown / Clock", "Paradox / Irony", "Microâ†’Macro", "David vs Goliath",
                 "Hidden Cost", "Archival Reframe"]
                if get_lang()=="English" else
                ["Ø´Ø®ØµÙŠØ© (Ù…Ù† Ø§Ù„Ø¯Ø§Ø®Ù„ Ù„Ù„Ø®Ø§Ø±Ø¬)", "Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„ÙØ±Ø¯", "ØªØ­Ù‚ÙŠÙ‚ / ØªØ³Ø±ÙŠØ¨", "ÙˆØµÙˆÙ„ Ø­ØµØ±ÙŠ",
                 "Ø¹Ø¯Ù‘ ØªÙ†Ø§Ø²Ù„ÙŠ / Ø§Ù„Ø³Ø§Ø¹Ø©", "Ù…ÙØ§Ø±Ù‚Ø© / Ø³Ø®Ø±ÙŠØ©", "Ù…Ù† Ø§Ù„ØµØºÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ¨ÙŠØ±", "Ø¯Ø§ÙˆØ¯ ÙˆØ¬Ø§Ù„ÙˆØª",
                 "Ø§Ù„ÙƒÙ„ÙØ© Ø§Ù„Ø®ÙÙŠØ©", "Ø¥Ø¹Ø§Ø¯Ø© ØªØ£Ø·ÙŠØ± Ø£Ø±Ø´ÙŠÙÙŠ"]
            ),
            default=None, key="log_angles"
        )

        target_len_short = st.slider(L("Target words for TAGLINE","Ø¹Ø¯Ø¯ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø¹Ø§Ø± Ø§Ù„Ù‚ØµÙŠØ±"), 8, 22, 15, key="log_len_short")
        target_len_detail = st.slider(L("Target words for DETAILED logline","Ø¹Ø¯Ø¯ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù„ÙˆØ¬Ù„Ø§ÙŠÙ† Ø§Ù„ØªÙØµÙŠÙ„ÙŠ"), 35, 90, 55, key="log_len_detail")
        add_angle_note = st.checkbox(L("Add one-line Angle Note (why this, why now)","Ø¥Ø¶Ø§ÙØ© Ø³Ø·Ø± ØªÙˆØ¶ÙŠØ­ÙŠ Ø¹Ù† Ø§Ù„Ø²Ø§ÙˆÙŠØ© (Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ ÙˆÙ„Ù…Ø§Ø°Ø§ Ø§Ù„Ø¢Ù†)"), value=True, key="log_add_note")
        n_variants = st.slider(L("How many variants?","Ø¹Ø¯Ø¯ Ø§Ù„ØµÙŠØºØŸ"), 3, 12, 6, key="log_n")

        anti_cliche = st.checkbox(L("Remove clichÃ©s (untold, explores, journeyâ€¦)","Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„ÙŠØ´ÙŠÙ‡Ø§Øª (ØºÙŠØ± Ù…Ø³Ø¨ÙˆÙ‚ØŒ ÙŠØ³ØªÙƒØ´ÙØŒ Ø±Ø­Ù„Ø©...)"), value=True, key="log_anticliche")
        freshness_nudge = st.checkbox(L("Nudge for freshness vs common AJD framings","ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø¬ÙØ¯Ø© Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©"), value=True, key="log_freshness")

        # Optionally use ChatGPT/OpenAI to generate loglines instead of the template-based method.
        use_ai = st.checkbox(
            L(
                "Use ChatGPT for loglines (requires OPENAI_API_KEY)",
                "Ø§Ø³ØªØ®Ø¯Ù… ChatGPT Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù„ÙˆØ¬Ù„Ø§ÙŠÙ† (ÙŠØªØ·Ù„Ø¨ OPENAI_API_KEY)"
            ),
            value=False,
            key="log_use_ai"
        )

        # Preset application (evaluated at generation time)
        def preset_defaults(name: str):
            if get_lang() == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                if name == "ÙˆÙŠØªÙ†Ø³ / Ø±ØµØ¯ÙŠ":
                    return {"tone": "Ù…Ø­Ø§ÙŠØ¯", "angles": ["Ø´Ø®ØµÙŠØ© (Ù…Ù† Ø§Ù„Ø¯Ø§Ø®Ù„ Ù„Ù„Ø®Ø§Ø±Ø¬)", "ÙˆØµÙˆÙ„ Ø­ØµØ±ÙŠ", "Ù…Ù† Ø§Ù„ØµØºÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ¨ÙŠØ±"], "short": 14, "detail": 60}
                if name == "ØªØ­Ù‚ÙŠÙ‚ÙŠ (ÙŠØ¹ØªÙ…Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚)":
                    return {"tone": "Ø¹Ø§Ø¬Ù„", "angles": ["ØªØ­Ù‚ÙŠÙ‚ / ØªØ³Ø±ÙŠØ¨", "Ø§Ù„ÙƒÙ„ÙØ© Ø§Ù„Ø®ÙÙŠØ©", "Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„ÙØ±Ø¯", "Ø¥Ø¹Ø§Ø¯Ø© ØªØ£Ø·ÙŠØ± Ø£Ø±Ø´ÙŠÙÙŠ"], "short": 14, "detail": 60}
                if name == "Ø´Ø¤ÙˆÙ† Ø¬Ø§Ø±ÙŠØ© (Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„ÙØ±Ø¯)":
                    return {"tone": "Ø¹Ø§Ø¬Ù„", "angles": ["Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„ÙØ±Ø¯", "Ø¹Ø¯Ù‘ ØªÙ†Ø§Ø²Ù„ÙŠ / Ø§Ù„Ø³Ø§Ø¹Ø©", "Ø¯Ø§ÙˆØ¯ ÙˆØ¬Ø§Ù„ÙˆØª"], "short": 15, "detail": 55}
                if name == "Ø£Ø±Ø´ÙŠÙÙŠ / Ø§Ù„Ù…Ø§Ø¶ÙŠ ÙÙŠ Ø§Ù„Ø­Ø§Ø¶Ø±":
                    return {"tone": "Ù…Ø­Ø§ÙŠØ¯", "angles": ["Ø¥Ø¹Ø§Ø¯Ø© ØªØ£Ø·ÙŠØ± Ø£Ø±Ø´ÙŠÙÙŠ", "Ù…Ù† Ø§Ù„ØµØºÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ¨ÙŠØ±"], "short": 15, "detail": 58}
                if name == "ÙˆØµÙˆÙ„ Ø­ØµØ±ÙŠ":
                    return {"tone": "Ù…Ø­Ø§ÙŠØ¯", "angles": ["ÙˆØµÙˆÙ„ Ø­ØµØ±ÙŠ", "Ø´Ø®ØµÙŠØ© (Ù…Ù† Ø§Ù„Ø¯Ø§Ø®Ù„ Ù„Ù„Ø®Ø§Ø±Ø¬)"], "short": 14, "detail": 55}
                if name == "Ø¹Ø¯Ù‘ ØªÙ†Ø§Ø²Ù„ÙŠ / Ø³Ø¨Ø§Ù‚ Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª":
                    return {"tone": "Ø¹Ø§Ø¬Ù„", "angles": ["Ø¹Ø¯Ù‘ ØªÙ†Ø§Ø²Ù„ÙŠ / Ø§Ù„Ø³Ø§Ø¹Ø©", "Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„ÙØ±Ø¯"], "short": 13, "detail": 50}
            else:
                if name == "Witness / Observational":
                    return {"tone": "Neutral", "angles": ["Character (inside-out)", "Exclusive Access", "Microâ†’Macro"], "short": 14, "detail": 60}
                if name == "Investigative (doc-forward)":
                    return {"tone": "Urgent", "angles": ["Investigation / Leak", "Hidden Cost", "System vs Individual", "Archival Reframe"], "short": 14, "detail": 60}
                if name == "Current Affairs (system vs individual)":
                    return {"tone": "Urgent", "angles": ["System vs Individual", "Countdown / Clock", "David vs Goliath"], "short": 15, "detail": 55}
                if name == "Archive-led / Past in Present":
                    return {"tone": "Neutral", "angles": ["Archival Reframe", "Microâ†’Macro"], "short": 15, "detail": 58}
                if name == "Exclusive Access":
                    return {"tone": "Neutral", "angles": ["Exclusive Access", "Character (inside-out)"], "short": 14, "detail": 55}
                if name == "Countdown / Clock":
                    return {"tone": "Urgent", "angles": ["Countdown / Clock", "System vs Individual"], "short": 13, "detail": 50}
            return None

        _pd = preset_defaults(preset)
        tone_used = _pd["tone"] if _pd else tone
        angles_used = _pd["angles"] if _pd else (angle_pack or [])
        short_used = _pd["short"] if _pd else target_len_short
        detail_used = _pd["detail"] if _pd else target_len_detail

        # ---- Helpers for loglines ----
        def _strip_cliche(text: str) -> str:
            if not anti_cliche:
                return text
            if get_lang() == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                repl = {
                    r"\bØºÙŠØ± Ù…Ø³Ø¨ÙˆÙ‚\b": "Ù†Ø§Ø¯Ø±",
                    r"\bÙŠØ³Ù„Ø· Ø§Ù„Ø¶ÙˆØ¡\b": "ÙŠÙƒØ´Ù",
                    r"\bÙŠØ³Ø¨Ø± Ø£ØºÙˆØ§Ø±\b": "ÙŠØºÙˆØµ Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ",
                    r"\bØ±Ø­Ù„Ø©\b": "ØµØ±Ø§Ø¹",
                    r"\bØ­Ù…ÙŠÙ…ÙŠ\b": "Ù‚Ø±ÙŠØ¨ Ø§Ù„Ù…Ù„Ø§Ù…Ø³",
                    r"\bÙ…Ø¤Ø«Ø±\b": "Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù…Ø®Ø§Ø·Ø±",
                }
            else:
                repl = {
                    r"\buntold\b": "unreported",
                    r"\bnever[- ]before[- ]seen\b": "rarely seen",
                    r"\bexplores\b": "cuts into",
                    r"\bjourney\b": "fight",
                    r"\bshines a light on\b": "exposes",
                    r"\bsheds light on\b": "exposes",
                    r"\bdelves into\b": "drives into",
                    r"\bheart[- ]wrenching\b": "stark",
                    r"\bgripping\b": "high-stakes",
                    r"\bintimate portrait\b": "close-quarters view",
                }
            out = text
            for pat, rep in repl.items():
                out = re.sub(pat, rep, out, flags=re.I)
            return out

        def _apply_tone(text: str, t: str) -> str:
            s = text
            if get_lang() == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                if t == "Ø¹Ø§Ø¬Ù„":
                    s = s.rstrip(".!ØŸ") + " Ø§Ù„Ø¢Ù†."
                elif t == "Ø®Ø´Ù†":
                    s = s.replace("Ø§Ø®ØªÙŠØ§Ø±", "Ù…Ø­Ø§Ø³Ø¨Ø©").replace("Ø³Ø±", "Ø³ÙˆÙ‚ Ø³ÙˆØ¯Ø§Ø¡")
                elif t == "Ø´Ø¹Ø±ÙŠ":
                    s = s.replace(" â€” ", "ØŒ ").replace(" Ø¶Ø¯ ", " Ø£Ù…Ø§Ù… ")
            else:
                if t == "Urgent":
                    s = s.rstrip(".") + ". Now."
                elif t == "Gritty":
                    s = s.replace("choice", "reckoning").replace("secret", "black-market")
                elif t == "Poetic":
                    s = s.replace(" â€” ", ", ").replace(" vs ", " against ")
            return s

        def _tighten_to_words(text: str, max_words: int) -> str:
            words = text.split()
            out = " ".join(words[:max_words]).rstrip(",;:â€¢â€” ")
            return out + "." if not out.endswith((".", "ØŸ")) else out

        def _fresh_nudge(seed_txt: str, base: str) -> str:
            if not freshness_nudge:
                return base
            hints_en = [
                " told through a single 36-hour window",
                " anchored in one apartment blockâ€™s WhatsApp audio",
                " using receipts, repair slips, and one leaked invoice",
                " from the perspective of the least powerful worker on shift",
                " limited to places where cameras usually arenâ€™t allowed",
                " where every claim must be proven on camera",
            ]
            hints_ar = [
                " ØªÙØ±ÙˆÙ‰ Ø¶Ù…Ù† Ù†Ø§ÙØ°Ø© Ø²Ù…Ù†ÙŠØ© ÙˆØ§Ø­Ø¯Ø© Ù…Ø¯ØªÙ‡Ø§ Ù£Ù¦ Ø³Ø§Ø¹Ø©",
                " ØªØ±ØªÙƒØ² Ø¹Ù„Ù‰ Ù…Ø°ÙƒØ±Ø§Øª ØµÙˆØªÙŠØ© ÙÙŠ ÙˆØ§ØªØ³Ø§Ø¨ Ù„Ø¹Ù…Ø§Ø±Ø© ÙˆØ§Ø­Ø¯Ø©",
                " Ø¨ÙˆØ«Ø§Ø¦Ù‚: Ø¥ÙŠØµØ§Ù„Ø§ØªØŒ ÙÙˆØ§ØªÙŠØ± ØµÙŠØ§Ù†Ø©ØŒ ÙˆÙØ§ØªÙˆØ±Ø© Ù…Ø³Ø±Ù‘Ø¨Ø©",
                " Ù…Ù† Ù…Ù†Ø¸ÙˆØ± Ø£Ø¶Ø¹Ù Ø¹Ø§Ù…Ù„ ÙÙŠ Ø§Ù„ÙˆØ±Ø¯ÙŠØ©",
                " Ø¨Ø­Ø¯ÙˆØ¯ Ø£Ù…Ø§ÙƒÙ† Ù„Ø§ ÙŠÙØ³Ù…Ø­ Ø¹Ø§Ø¯Ø© Ø¨Ø§Ù„ØªØµÙˆÙŠØ± ÙÙŠÙ‡Ø§",
                " Ø­ÙŠØ« ÙŠØ¬Ø¨ Ø¥Ø«Ø¨Ø§Øª ÙƒÙ„ Ø§Ø¯Ù‘Ø¹Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§",
            ]
            hints = hints_ar if get_lang()=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else hints_en
            low = base.lower()
            triggers = ["36-hour","whatsapp","leaked","invoice","proven","Ø¹Ù…Ø§Ø±Ø©","Ù…Ø³Ø±Ù‘Ø¨Ø©","Ù£Ù¦"]
            if any(t in low for t in triggers):
                return base
            return base.rstrip(".ØŸ") + hints[hash(seed_txt) % len(hints)] + "."

        def _angle_lines(seed_txt: str):
            s = seed_txt.strip()
            if get_lang() == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                return [
                    ("Ø´Ø®ØµÙŠØ© (Ù…Ù† Ø§Ù„Ø¯Ø§Ø®Ù„ Ù„Ù„Ø®Ø§Ø±Ø¬)", f"Ø¹Ù†Ø¯Ù…Ø§ ÙŠØªÙ‚Ø§Ø·Ø¹ {s} Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ù„Ù… ÙŠØ¹Ø¯ ÙŠØ­ØªÙ…Ù„Ù‡Ø§ØŒ Ù‚Ø±Ø§Ø±ÙŒ ÙˆØ§Ø­Ø¯ ÙŠØ¹ÙŠØ¯ Ø±Ø³Ù… Ø§Ù„Ø­Ø¯Ù‘ Ø¨ÙŠÙ† Ø­ÙØ¸ Ø§Ù„ÙˆØ¬Ù‡ ÙˆØ¥Ù†Ù‚Ø§Ø° Ø§Ù„Ø£Ø±ÙˆØ§Ø­."),
                    ("Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„ÙØ±Ø¯", f"Ø¹Ø§Ù…Ù„ ÙÙŠ Ø§Ù„ØµÙÙ‘ Ø§Ù„Ø£ÙˆÙ„ Ø¹Ø§Ù„Ù‚ Ø¯Ø§Ø®Ù„ {s} ÙŠØ®ØªØ¨Ø± Ø¥Ù„Ù‰ Ø£ÙŠ Ø­Ø¯Ù‘ ÙŠÙ†Ø«Ù†ÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ù‚Ø¨Ù„ Ø£Ù† ÙŠÙ†ÙƒØ³Ø± â€” ÙˆÙ…Ù† ÙŠØ¯ÙØ¹ Ø­ÙŠÙ† Ù„Ø§ ÙŠÙ†Ø«Ù†ÙŠ."),
                    ("ØªØ­Ù‚ÙŠÙ‚ / ØªØ³Ø±ÙŠØ¨", f"ØªØ³Ø±ÙŠØ¨ÙŒ ÙŠÙØªØ­ {s}ØŒ ÙˆØ§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„ÙˆØ±Ù‚ÙŠØ© ØªÙØ±Ø¶ Ø®ÙŠØ§Ø±Ù‹Ø§: ÙƒØ´Ù Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ù… Ø§Ù„Ø¨Ù‚Ø§Ø¡ Ø­Ù„Ù‚Ø© ØµØ§Ù…ØªØ©."),
                    ("ÙˆØµÙˆÙ„ Ø­ØµØ±ÙŠ", f"Ø¯Ø§Ø®Ù„ {s} ØªØ¨Ù‚Ù‰ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø¯Ø§Ø¦Ø±Ø© Ø­ÙŠØ« ØªÙØ·ÙØ£ Ø¹Ø§Ø¯Ø©: Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø­ÙŠÙ† Ù„Ø§ ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø­ÙØ±Ù‘Ø§Ø³ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©."),
                    ("Ø¹Ø¯Ù‘ ØªÙ†Ø§Ø²Ù„ÙŠ / Ø§Ù„Ø³Ø§Ø¹Ø©", f"ÙÙŠ {s} ÙƒÙ„ Ø³Ø§Ø¹Ø© ØªØ±ÙØ¹ Ø«Ù…Ù† Ø§Ù„ØµÙ…Øª â€” Ø­ØªÙ‰ ÙŠØ­ÙŠÙ† Ø§Ù„Ù…ÙˆØ¹Ø¯ ÙˆÙŠØºÙŠØ¨ Ø§Ù„Ø£ÙƒØ³Ø¬ÙŠÙ†."),
                    ("Ù…ÙØ§Ø±Ù‚Ø© / Ø³Ø®Ø±ÙŠØ©", f"{s} ÙŠØ­ÙˆÙ‘Ù„ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¥Ù„Ù‰ Ø®Ø·Ø±: Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ø°ÙŠ ÙŠØ­Ù…ÙŠ Ø¨Ø·Ù„Ù‡ ÙŠØ¬Ø¹Ù„Ù‡ Ù‡Ø¯ÙÙ‹Ø§."),
                    ("Ù…Ù† Ø§Ù„ØµØºÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ¨ÙŠØ±", f"ØªÙØµÙŠÙ„ ØµØºÙŠØ± Ø¯Ø§Ø®Ù„ {s} ÙŠØ±Ø³Ù… Ø®Ø±Ø§Ø¦Ø· Ø£Ø²Ù…Ø© Ø£ÙƒØ¨Ø± â€” ÙƒÙŠÙ ØªØ­Ø±Ù‘Ùƒ Ø§Ù„Ø£ÙŠØ¯ÙŠ Ø§Ù„ØµØºÙŠØ±Ø© Ø¢Ù„Ø©Ù‹ ÙƒØ¨ÙŠØ±Ø©."),
                    ("Ø¯Ø§ÙˆØ¯ ÙˆØ¬Ø§Ù„ÙˆØª", f"ÙÙŠ Ù…ÙˆØ§Ø¬Ù‡Ø© {s} ÙŠØ¬Ø¯ Ø¹Ø§Ù…Ù„ Ù…ÙÙ†Ù‡Ùƒ Ø§Ù„Ø³Ù„Ø§Ø­ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ Ù„Ø§ ÙŠØµØ¯Ù‘Ù‡ Ø¬Ø§Ù„ÙˆØª: Ø§Ù„Ø¯Ù„ÙŠÙ„."),
                    ("Ø§Ù„ÙƒÙ„ÙØ© Ø§Ù„Ø®ÙÙŠØ©", f"{s} ÙŠØ¨Ø¯Ùˆ Ø£Ø±Ø®Øµ Ù…Ù† Ø§Ù„Ø­Ù„ â€” Ø­ØªÙ‰ ØªØµÙ„ Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ø¨Ø§Ù„Ø£Ø¬Ø³Ø§Ø¯ ÙˆØ§Ù„Ø±Ø®ØµØ© ÙˆØ§Ù„Ù†ÙˆÙ…."),
                    ("Ø¥Ø¹Ø§Ø¯Ø© ØªØ£Ø·ÙŠØ± Ø£Ø±Ø´ÙŠÙÙŠ", f"Ø¨Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ§Ù„Ø¥ÙŠØµØ§Ù„Ø§Øª ÙˆØ§Ù„Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ù…Ø®Ø¨Ù‘Ø£Ø© ÙŠØ¹ÙŠØ¯ {s} ØªØ´ØºÙŠÙ„ ÙØ¶ÙŠØ­Ø© ÙƒØ§Ù†Øª Ø£Ù…Ø§Ù… Ø§Ù„Ø¹ÙŠÙˆÙ†."),
                ]
            else:
                return [
                    ("Character (inside-out)", f"When {s} collides with a rule he can no longer obey, one decision redraws the line between saving face and saving lives."),
                    ("System vs Individual", f"A frontline worker trapped inside {s} tests how far a system will bend before it breaksâ€”and who pays when it doesnâ€™t."),
                    ("Investigation / Leak", f"A leak cracks open {s}, and the paper trail forces a choice: expose the scheme or become another silent link."),
                    ("Exclusive Access", f"Inside {s}, cameras keep rolling where theyâ€™re usually shut: what happens when the gatekeepers canâ€™t edit the truth."),
                    ("Countdown / Clock", f"In {s}, each hour raises the price of staying quietâ€”until the deadline hits and someoneâ€™s oxygen runs out."),
                    ("Paradox / Irony", f"{s} turns attention into risk: the audience that makes the hero untouchable also makes him a target."),
                    ("Microâ†’Macro", f"A tiny decision inside {s} maps the wiring of a bigger crisisâ€”how small hands move a large machine."),
                    ("David vs Goliath", f"Against {s}, one underpaid insider finds the only weapon Goliath canâ€™t block: proof."),
                    ("Hidden Cost", f"{s} looks cheaper than the fixâ€”until the bill arrives in bodies, licenses, and sleep."),
                    ("Archival Reframe", f"Using old calls, receipts and cached clips, {s} plays back a scandal that was hiding in plain sight."),
                ]

        def _expand_to_detailed(seed_txt: str, tone_used: str, max_words: int) -> str:
            if get_lang() == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                who = (seed_txt.strip().rstrip(".!ØŸ") + ". ") if seed_txt.strip() else ""
                stakes = "ÙˆÙ…Ø¹ Ø§Ù‚ØªØ±Ø§Ø¨ Ø§Ù„Ø®ÙŠØ·ØŒ ÙŠØ¬Ø¯ Ø§Ù„Ø¨Ø·Ù„ Ù†ÙØ³Ù‡ Ø¨ÙŠÙ† Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù†Ø§Ø³ Ø£Ùˆ Ø­Ù…Ø§ÙŠØ© Ù†ÙØ³Ù‡ â€” ÙˆÙƒÙÙ„Ø§Ù‡Ù…Ø§ Ø«Ù…Ù†Ù‡ ÙƒØ§Ø±Ø«Ø© Ù…Ù‡Ù†ÙŠØ© Ø£Ùˆ Ø¥Ù†Ø³Ø§Ù†ÙŠØ©. "
                angle = "Ø¨Ù„Ù‚Ø·Ø§Øª Ù‚Ø±ÙŠØ¨Ø© ÙˆØ¯Ù„ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø©ØŒ ÙŠØ®ØªØ¨Ø± Ø§Ù„ÙÙŠÙ„Ù… ÙƒÙŠÙ ØªØ¨Ø¯Ùˆ Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø­ÙŠÙ† ÙŠÙØ¶Ù‘Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØµÙ…Øª."
                out = (who + stakes + angle).strip()
            else:
                who = (seed_txt.strip().rstrip(".") + ". ") if seed_txt.strip() else ""
                stakes = "As the trail sharpens, the protagonist must choose between protecting people and protecting themselves â€” either path could end a career or a life. "
                angle  = "Shot with close-quarters access and proof-led scenes, the film tests what truth looks like when the system prefers silence."
                out = (who + stakes + angle).strip()
            out = _strip_cliche(out)
            out = _apply_tone(out, tone_used)
            out = _fresh_nudge(seed_txt, out)
            out = _tighten_to_words(out, max_words)
            return out

        def build_loglines(seed_txt: str, tone_used: str, angles: List[str], n: int, w_short: int, w_detail: int, add_note: bool):
            pool = [p for p in _angle_lines(seed_txt) if not angles or p[0] in angles]
            if not pool:
                pool = _angle_lines(seed_txt)
            out_rows = []
            i = 0
            while len(out_rows) < n:
                label, draft = pool[i % len(pool)]
                tagline = _tighten_to_words(_fresh_nudge(seed_txt, _apply_tone(_strip_cliche(draft), tone_used)), w_short)
                detailed = _expand_to_detailed(seed_txt, tone_used, w_detail)

                if add_note:
                    if get_lang() == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                        notes = {
                            "Ø´Ø®ØµÙŠØ© (Ù…Ù† Ø§Ù„Ø¯Ø§Ø®Ù„ Ù„Ù„Ø®Ø§Ø±Ø¬)": "Ø²Ø§ÙˆÙŠØ©: Ù‚ÙŠØ§Ø¯Ø© Ø´Ø®ØµÙŠØ©ØŒ Ù‚Ø±Ø§Ø±Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ØŒ Ù…ØµØ¯Ø§Ù‚ÙŠØ© Ù…Ù† Ø§Ù„ÙˆØµÙˆÙ„.",
                            "Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„ÙØ±Ø¯": "Ø²Ø§ÙˆÙŠØ©: Ù…Ù† Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ø³Ø¯Ø› Ø§Ù„Ø¹Ø§Ù‚Ø¨Ø© ØªÙ‚Ø¹ Ø¹Ù„Ù‰ ÙØ±Ø¯.",
                            "ØªØ­Ù‚ÙŠÙ‚ / ØªØ³Ø±ÙŠØ¨": "Ø²Ø§ÙˆÙŠØ©: Ø£Ø«Ø± ÙˆØ«Ø§Ø¦Ù‚ÙŠ ÙˆÙ…Ù‚Ø§Ø·Ø¹ ØµÙˆØªØ› Ù…Ø´Ø§Ù‡Ø¯ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚.",
                            "ÙˆØµÙˆÙ„ Ø­ØµØ±ÙŠ": "Ø²Ø§ÙˆÙŠØ©: ÙƒØ§Ù…ÙŠØ±Ø§Øª ÙÙŠ ØºØ±Ù Ù…Ù…Ù†ÙˆØ¹Ø©Ø› ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©.",
                            "Ø¹Ø¯Ù‘ ØªÙ†Ø§Ø²Ù„ÙŠ / Ø§Ù„Ø³Ø§Ø¹Ø©": "Ø²Ø§ÙˆÙŠØ©: Ø¶ØºØ· Ø²Ù…Ù†ÙŠØ› ÙÙˆØ§ØµÙ„ Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ø¹ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯.",
                            "Ù…ÙØ§Ø±Ù‚Ø© / Ø³Ø®Ø±ÙŠØ©": "Ø²Ø§ÙˆÙŠØ©: Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ ÙƒØ¹Ø¨Ø¡ Ø£Ù…Ù†ÙŠØ› Ø§Ù„Ø£Ù…Ø§Ù† Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø¸Ù‡ÙˆØ±.",
                            "Ù…Ù† Ø§Ù„ØµØºÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ¨ÙŠØ±": "Ø²Ø§ÙˆÙŠØ©: Ø­Ø§Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ØªØ´Ø±Ø­ Ø¨Ù†ÙŠØ© Ø£Ø²Ù…Ø© Ø£ÙˆØ³Ø¹.",
                            "Ø¯Ø§ÙˆØ¯ ÙˆØ¬Ø§Ù„ÙˆØª": "Ø²Ø§ÙˆÙŠØ©: Ù„Ø§ ØªÙƒØ§ÙØ¤ Ù‚ÙˆØ©Ø› Ø§Ù„Ø³Ù„Ø§Ø­ Ù‡Ùˆ Ø§Ù„Ø¯Ù„ÙŠÙ„.",
                            "Ø§Ù„ÙƒÙ„ÙØ© Ø§Ù„Ø®ÙÙŠØ©": "Ø²Ø§ÙˆÙŠØ©: ØªÙƒØ§Ù„ÙŠÙ ØºÙŠØ± Ù…Ø±Ø¦ÙŠØ© ØªØµØ¨Ø­ Ù…Ø­Ø³ÙˆØ³Ø©.",
                            "Ø¥Ø¹Ø§Ø¯Ø© ØªØ£Ø·ÙŠØ± Ø£Ø±Ø´ÙŠÙÙŠ": "Ø²Ø§ÙˆÙŠØ©: Ø§Ù„Ù…Ø§Ø¶ÙŠ ÙŠØ¹ÙŠØ¯ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø­Ø§Ø¶Ø±.",
                        }
                    else:
                        notes = {
                            "Character (inside-out)": "Angle: character-led; decisions on camera; access-driven credibility.",
                            "System vs Individual": "Angle: policy-to-person pipeline; consequence on one body.",
                            "Investigation / Leak": "Angle: document/voice-note trail; verifiable scenes.",
                            "Exclusive Access": "Angle: cameras in no-go rooms; procedural realism.",
                            "Countdown / Clock": "Angle: time pressure with natural act breaks.",
                            "Paradox / Irony": "Angle: visibility as liability; safety vs fame.",
                            "Microâ†’Macro": "Angle: one case explains the system; blueprint of a wider crisis.",
                            "David vs Goliath": "Angle: power asymmetry; weapon is proof.",
                            "Hidden Cost": "Angle: externalities made visible; bodies/time as currency.",
                            "Archival Reframe": "Angle: past re-arranges the present; archive-forward.",
                        }
                    note = notes.get(label, "")
                else:
                    note = ""

                out_rows.append({"angle": label, "tagline": tagline, "detailed": detailed, "note": note})
                i += 1
            return out_rows

        if st.button(L("Generate loglines","Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù„ÙˆØ¬Ù„Ø§ÙŠÙ†"), type="primary", key="log_gen_btn"):
            if not seed.strip():
                st.warning(L("Please add a short seed.","Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ ÙˆØµÙ Ù‚ØµÙŠØ± Ø£ÙˆÙ„Ø§Ù‹."))
            else:
                # If the user opted to use AI and prerequisites are met, try to generate loglines via OpenAI
                if use_ai:
                    if not OPENAI_AVAILABLE:
                        st.error(L(
                            "The openai package is not installed. Please ensure it is listed in requirements.txt.",
                            "Ø­Ø²Ù…Ø© openai ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¶Ø§ÙØªÙ‡Ø§ ÙÙŠ Ù…Ù„Ù Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª requirements.txt."
                        ))
                    else:
                        api_key = os.getenv("OPENAI_API_KEY")
                        if not api_key:
                            st.error(L(
                                "Environment variable OPENAI_API_KEY is not set. Please set it before using ChatGPT.",
                                "Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© OPENAI_API_KEY ØºÙŠØ± Ù…Ø¹Ø±Ù‘Ù. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¶Ø¨Ø·Ù‡ Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… ChatGPT."
                            ))
                        else:
                            angles_str = ", ".join(angles_used) if angles_used else ""
                            prompt_en = (
                                "You are a creative writer who crafts short, compelling loglines for documentary film proposals "
                                "in both English and Arabic. "
                                f"Generate {n_variants} variants for the following film description. "
                                "Each variant should include an English tagline (" 
                                f"<= {short_used} words), an Arabic tagline (<= {short_used} words), "
                                "an English detailed logline (" 
                                f"<= {detail_used} words), and an Arabic detailed logline "
                                f"(<= {detail_used} words). "
                                f"Film description: {seed.strip()}. Tone: {tone_used}. Angles: {angles_str}. "
                                "Output each variant as four lines in the order: English tagline, Arabic tagline, "
                                "English detailed, Arabic detailed, separated by newlines. Separate variants with a blank line."
                            )
                            try:
                                openai.api_key = api_key
                                resp = openai.ChatCompletion.create(
                                    model="gpt-4o",
                                    messages=[
                                        {"role": "system",
                                         "content": "You are an assistant that writes bilingual loglines for documentary proposals."},
                                        {"role": "user", "content": prompt_en},
                                    ],
                                    max_tokens=800,
                                    temperature=0.7,
                                )
                                content = resp.choices[0].message.get("content", "").strip()
                                if not content:
                                    st.info(L(
                                        "No output returned from OpenAI.",
                                        "Ù„Ù… ÙŠØ±Ø¬Ø¹ OpenAI Ø£ÙŠ Ù…Ø®Ø±Ø¬Ø§Øª."
                                    ))
                                else:
                                    st.markdown(L("### AI-generated loglines","### Ù„ÙˆØ¬Ù„Ø§ÙŠÙ† Ù…Ù†Ø´Ø£ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"))
                                    variants = [block.strip() for block in content.split("\n\n") if block.strip()]
                                    for v in variants:
                                        lines = [ln.strip() for ln in v.split("\n") if ln.strip()]
                                        st.divider()
                                        if len(lines) >= 1:
                                            st.write(f"**{L('English Tagline','Ø§Ù„Ø´Ø¹Ø§Ø± Ø§Ù„Ù‚ØµÙŠØ± Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©')}**: {lines[0]}")
                                        if len(lines) >= 2:
                                            st.write(f"**{L('Arabic Tagline','Ø§Ù„Ø´Ø¹Ø§Ø± Ø§Ù„Ù‚ØµÙŠØ± Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©')}**: {lines[1]}")
                                        if len(lines) >= 3:
                                            st.write(f"**{L('English Detailed','Ø§Ù„Ù„ÙˆØ¬Ù„Ø§ÙŠÙ† Ø§Ù„ØªÙØµÙŠÙ„ÙŠ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©')}**: {lines[2]}")
                                        if len(lines) >= 4:
                                            st.write(f"**{L('Arabic Detailed','Ø§Ù„Ù„ÙˆØ¬Ù„Ø§ÙŠÙ† Ø§Ù„ØªÙØµÙŠÙ„ÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©')}**: {lines[3]}")
                            except Exception as e:
                                st.error(L(
                                    f"Error from OpenAI: {e}",
                                    f"Ø®Ø·Ø£ Ù…Ù† OpenAI: {e}"
                                ))
                # If AI not used, fall back to the template-based logline generation
                else:
                    rows = build_loglines(seed, tone_used, angles_used, n_variants, short_used, detail_used, add_angle_note)
                    if not rows:
                        st.info(L("No output. Try different angles or increase word targets.","Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ø§ØªØ¬. Ø¬Ø±Ù‘Ø¨ Ø²ÙˆØ§ÙŠØ§ Ø£Ø®Ø±Ù‰ Ø£Ùˆ Ø²Ø¯ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª."))
                    else:
                        for idx, r in enumerate(rows, 1):
                            st.markdown(f"### {idx}. {r['angle']}")
                            st.write(f"**{L('Tagline','Ø§Ù„Ø´Ø¹Ø§Ø± Ø§Ù„Ù‚ØµÙŠØ±')}:** {r['tagline']}")
                            st.write(f"**{L('Detailed','Ø§Ù„Ù„ÙˆØ¬Ù„Ø§ÙŠÙ† Ø§Ù„ØªÙØµÙŠÙ„ÙŠ')}:** {r['detailed']}")
                            if add_angle_note and r["note"]:
                                st.caption(r["note"])
                            st.divider()
                        import io
                        buf = io.StringIO()
                        for idx, r in enumerate(rows, 1):
                            buf.write(f"{idx}. {r['angle']}\n")
                            buf.write(f"   {L('Tagline','Ø§Ù„Ø´Ø¹Ø§Ø±')}: {r['tagline']}\n")
                            buf.write(f"   {L('Detailed','ØªÙØµÙŠÙ„ÙŠ')}: {r['detailed']}\n")
                            if add_angle_note and r["note"]:
                                buf.write(f"   {L('Note','Ù…Ù„Ø§Ø­Ø¸Ø©')}: {r['note']}\n\n")
                        st.download_button(L("Download all loglines (.txt)","ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù„ÙˆØ¬Ù„Ø§ÙŠÙ†Ø§Øª (.txt)"),
                                           buf.getvalue().encode("utf-8"),
                                           file_name="loglines_bilingual.txt", key="log_dl_bi")

    # ============================ TAB 5: Diagnostics ============================
    with diag_tab:
        st.subheader(L("Diagnostics","Ø§Ù„ØªØ´Ø®ÙŠØµ"))
        files = sorted(glob.glob(str(DATA_DIR / "*")))
        st.write(L("**/data files**:","**Ù…Ù„ÙØ§Øª /data**:"), files)

        p_cat = DATA_DIR / "ajd_catalogue_raw.csv"
        if p_cat.exists():
            st.write(L("Catalogue file size:","Ø­Ø¬Ù… Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³:"), f"{p_cat.stat().st_size:,} {L('bytes','Ø¨Ø§ÙŠØª')}")
        else:
            st.info(L("Catalogue merged CSV missing","Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…ÙˆØ­Ù‘Ø¯ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯"))

        cat_df_preview = load_catalogue_df()
        st.write(L("Catalogue DataFrame shape:","Ø£Ø¨Ø¹Ø§Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙÙ‡Ø±Ø³:"), f"{cat_df_preview.shape[0]:,} Ã— {cat_df_preview.shape[1]}")
        if not cat_df_preview.empty:
            st.write(L("**Column names (first 30):**","**Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (Ø£ÙˆÙ„ 30):**"), list(cat_df_preview.columns[:30]))
            st.write(L("**Head(10):**","**Ø£ÙˆÙ„ 10 ØµÙÙˆÙ:**"))
            st.dataframe(cat_df_preview.head(10), use_container_width=True)
            texty = [c for c in cat_df_preview.columns if re.search(r"(name|title|synopsis|series|english|arabic|desc|topic|Ø§Ø³Ù…|Ø¹Ù†ÙˆØ§Ù†|Ù…Ù„Ø®|Ø¹Ø±Ø¨ÙŠ|Ø³Ù„Ø³Ù„Ø©|Ù…ÙˆØ¶ÙˆØ¹)", c, re.I)]
            st.write(L("**Guessed text columns:**","**Ø£Ø¹Ù…Ø¯Ø© Ù†ØµÙŠØ© Ù…Ø­ØªÙ…Ù„Ø©:**"), texty if texty else L("(none)","(Ù„Ø§ ÙŠÙˆØ¬Ø¯)"))
        else:
            st.warning(L("DataFrame is empty after loading. This usually means delimiter/encoding issues or an empty file.",
                         "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø±ØºØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„â€”ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø´ÙƒÙ„Ø© ÙØ§ØµÙ„/ØªØ±Ù…ÙŠØ² Ø£Ùˆ Ù…Ù„Ù ÙØ§Ø±Øº."))

        st.markdown(L("### Column preset for Search tab","### Ø¶Ø¨Ø· Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¨Ø­Ø«"))
        if not cat_df_preview.empty:
            _ = st.multiselect(
                L("Columns to search in (preset for Search tab)","Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ Ø§Ù„Ø¨Ø­Ø« ÙÙŠÙ‡Ø§ (ÙŠÙØ³ØªØ®Ø¯Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§)"),
                options=list(cat_df_preview.columns),
                default=texty or list(cat_df_preview.columns)[:5],
                key="diag_cols_preset",
            )
            st.caption(L("This preset is saved in session; the Search tab will use it automatically.",
                         "Ø³ÙŠÙØ­ÙØ¸ Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ÙÙŠ Ø§Ù„Ø¬Ù„Ø³Ø©Ø› ÙˆØ³ÙŠØ³ØªØ®Ø¯Ù…Ù‡ ØªØ¨ÙˆÙŠØ¨ Ø§Ù„Ø¨Ø­Ø« ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§."))
        else:
            st.info(L("Load a catalogue first to configure the column preset.","Ø­Ù…Ù‘Ù„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø£ÙˆÙ„Ù‹Ø§ Ù„Ø¶Ø¨Ø· Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¨Ø­Ø«."))

        st.markdown(L("### Upload / Replace catalogue CSV (session-only)","### Ø±ÙØ¹/Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³ (Ø¬Ù„Ø³Ø© ÙÙ‚Ø·)"))
        up = st.file_uploader(L("Upload a full catalogue CSV (UTF-8). Not persisted on redeploy.",
                                "Ø§Ø±ÙØ¹ Ù…Ù„Ù CSV ÙƒØ§Ù…Ù„ (UTF-8). Ù„Ù† ÙŠÙØ­ÙØ¸ Ø¨Ø¹Ø¯ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù†Ø´Ø±."),
                              type=["csv"], key="diag_upload_catalogue")
        if up is not None:
            os.makedirs(DATA_DIR, exist_ok=True)
            with open(DATA_DIR / "ajd_catalogue_raw.csv", "wb") as f:
                f.write(up.getbuffer())
            load_catalogue_df.clear()
            st.success(L("Uploaded. Click **Reload data / clear cache** in the sidebar, then reopen Diagnostics.",
                         "ØªÙ… Ø§Ù„Ø±ÙØ¹. Ø§Ø¶ØºØ· **ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª / Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©** Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØŒ Ø«Ù… Ø§ÙØªØ­ Ø§Ù„ØªØ´Ø®ÙŠØµ."))

        st.markdown(L("### Maintenance","### ØµÙŠØ§Ù†Ø©"))
        force_merge = st.button(L("Force re-merge from parts (ajd_catalogue_raw.part*.csv)","Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¯Ù…Ø¬ Ù…Ù† Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡"), key="diag_force_merge_btn")
        show_parts = st.button(L("Show part files","Ø¹Ø±Ø¶ Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡"), key="diag_show_parts_btn")
        if force_merge:
            merged = DATA_DIR / "ajd_catalogue_raw.csv"
            try:
                if merged.exists(): merged.unlink()
                ok = _merge_chunked_csv(str(DATA_DIR / "ajd_catalogue_raw.part*.csv"), merged)
                load_catalogue_df.clear()
                if ok:
                    st.success(L("Re-merged successfully. Click **Reload data / clear cache** in the sidebar.",
                                 "ØªÙ… Ø§Ù„Ø¯Ù…Ø¬ Ø¨Ù†Ø¬Ø§Ø­. Ø§Ø¶ØºØ· **ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª / Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©** Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ."))
                else:
                    st.warning(L("No part files found matching pattern.","Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ø£Ø¬Ø²Ø§Ø¡ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù†Ù…Ø·."))
            except Exception as e:
                st.error(L(f"Re-merge failed: {e}", f"ÙØ´Ù„ Ø§Ù„Ø¯Ù…Ø¬: {e}"))
        if show_parts:
            st.write(sorted(glob.glob(str(DATA_DIR / "ajd_catalogue_raw.part*.csv"))))

    st.markdown("---")
    st.caption(L(
        "Â© 2025 ICON Studio â€” AJD Topic Explorer Dashboard.",
        "Â© 2025 ICON Studio â€” Ù„ÙˆØ­Ø© Ø§Ø³ØªÙƒØ´Ø§Ù Ù…ÙˆØ¶ÙˆØ¹Ø§Øª AJD."
    ))

if __name__ == "__main__":
    main()
